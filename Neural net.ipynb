{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting train/valid/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('dataset.npz', allow_pickle=True)\n",
    "ingredients = data['ingredients']\n",
    "recipes = data['recipes']\n",
    "vectorized_len = np.vectorize(len)\n",
    "recipes = recipes[vectorized_len(recipes) > 0]\n",
    "\n",
    "def split_train_val_test(recipes, train=0.8, val=0.1):\n",
    "    shuffled = np.random.RandomState(0).permutation(recipes)\n",
    "    n_train = int(len(shuffled) * train)\n",
    "    n_val = int(len(shuffled) * val)\n",
    "    return shuffled[:n_train], shuffled[n_train: n_train + n_val], shuffled[-n_val:]\n",
    "\n",
    "train_recipes, val_recipes, test_recipes = split_train_val_test(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 219,  212,   46, 1133,  222,  657, 1343, 1014,   73,  140,   26,\n",
       "          8,  286]),\n",
       "       array([  77,  967, 3002,  199,    2,    3,   98,   18, 1477,   25,  895,\n",
       "          1,  157,    0,    8]),\n",
       "       array([  14,  134,  147,    3,   33,    9,   30, 3275,    1,   73,  511,\n",
       "       1597]),\n",
       "       array([198, 233,  14,   3,  33,  42, 120, 151,  10,   7,   1,   0,  21,\n",
       "        26]),\n",
       "       array([167,  52,  32,  13,   5, 224,  71,   0,  43,   4,  36])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_recipes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert our data into one-hot encoding\n",
    "# we'll also return the x vector (all - 1 ingredient) and y (missing ingredient)\n",
    "def convert_one_hot(array):\n",
    "    # here i'm getting an array of zeros\n",
    "    # num rows is the size of the input array (ie how many recipes)\n",
    "    # num cols is num of ingredients total (so we can 1-hot them)\n",
    "    one_hot = np.zeros((len(array), len(ingredients)))\n",
    "    inputs = np.zeros((len(array), len(ingredients)))\n",
    "    targets = np.empty(len(array))\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        if len(array[i]) > 0:\n",
    "            # this is just indexing into the ith row of the array (ith recipe)\n",
    "            # and saying all the values in the recipe we're gonna set to 1\n",
    "            one_hot[i][array[i]] = 1\n",
    "            \n",
    "            # randomly choose one of the ingredients\n",
    "            leave_out_idx = np.random.randint(len(array[i]))\n",
    "            leave_out = array[i][leave_out_idx]\n",
    "            leave_out_array = np.delete(array[i], leave_out_idx)\n",
    "            inputs[i][leave_out_array] = 1\n",
    "            targets[i] = leave_out\n",
    "            \n",
    "        else:\n",
    "            print(\"shouldn't get here ever\")\n",
    "        \n",
    "    return one_hot, inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot, train_x, train_y = convert_one_hot(train_recipes)\n",
    "val_one_hot, val_x, val_y = convert_one_hot(val_recipes)\n",
    "test_one_hot, test_x, test_y = convert_one_hot(test_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_size = 100\n",
    "x_mini = torch.tensor(train_x[:mini_size].astype(np.float32))\n",
    "y_mini = torch.tensor(train_y[:mini_size].astype(np.int_))\n",
    "mini_train_tensor = data_utils.TensorDataset(x_mini, y_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(train_x.astype(np.float32))\n",
    "y_train = torch.tensor(train_y.astype(np.int_))\n",
    "train_tensor = data_utils.TensorDataset(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = torch.tensor(val_x.astype(np.float32))\n",
    "y_valid = torch.tensor(val_y.astype(np.int_))\n",
    "valid_tensor = data_utils.TensorDataset(x_valid, y_valid) \n",
    "\n",
    "x_test = torch.tensor(test_x.astype(np.float32))\n",
    "y_test = torch.tensor(test_y.astype(np.int_))\n",
    "test_tensor = data_utils.TensorDataset(x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, params, num_ingredients):\n",
    "        super(Net, self).__init__()\n",
    "        self.params = params\n",
    "        self.fc0 = nn.Linear(num_ingredients, self.params[\"hidden_1\"])\n",
    "        self.fc1 = nn.Linear(self.params[\"hidden_1\"], self.params[\"hidden_2\"])\n",
    "        self.fc2 = nn.Linear(self.params[\"hidden_2\"], num_ingredients)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    sum_num_correct = 0\n",
    "    sum_loss = 0\n",
    "    num_batches_since_log = 0\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    batches = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    batches.set_description(\"Epoch NA: Loss (NA) Accuracy (NA %)\")\n",
    "    for batch_idx, (data, target) in batches:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(output, target)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "#         print(f\"Prediction: {pred}, Actual: {target}, Loss: {loss}\")\n",
    "        correct = pred.eq(target.float().view_as(pred)).sum().item()\n",
    "        sum_num_correct += correct\n",
    "        sum_loss += loss.item()\n",
    "        num_batches_since_log += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        batches.set_description(\n",
    "          \"Epoch {:d}: Loss ({:.2e},  Accuracy ({:02.0f}%)\".format(\n",
    "            epoch, loss.item(), 100. * sum_num_correct / (num_batches_since_log * train_loader.batch_size))\n",
    "        )\n",
    "        \n",
    "    return sum(losses)/len(losses)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # We use reduction = 'sum' here to ignore the impact of batch size and make \n",
    "            # this value comparable with the loss reported in the train loop above. Note,\n",
    "            # though, that we divide by the len of the dataset below (so this is truly a per-element loss value)\n",
    "#             test_loss += F.mse_loss(torch.clamp(output.view(target.shape), 0., 5.), target.float(), reduction='sum') # sum up the mean square loss\n",
    "#             pred = torch.clamp(output, 0., 5.)\n",
    "#             correct += pred.eq(target.float().view_as(pred)).sum().item()\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = torch.sum(loss_fn(output, target, reduction='sum')).item() # sum up batch loss\n",
    "            test_loss += loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "#     print('\\nTest set: Average loss: {:.2e}\\n'.format(test_loss))\n",
    "    print('\\nTest set: Average loss: {:.2e}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args[\"seed\"] = 1\n",
    "args[\"no_cuda\"] = False\n",
    "args[\"log_interval\"] = 50\n",
    "args[\"batch_size\"] = 8\n",
    "args[\"test-batch-size\"] = 1000\n",
    "\n",
    "params = dict()\n",
    "params[\"epochs\"] = 10\n",
    "params[\"lr\"] = 5e-2\n",
    "params[\"hidden_1\"] = len(ingredients)*2\n",
    "params[\"hidden_2\"] = len(ingredients)*2\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "mini_train_loader = torch.utils.data.DataLoader(\n",
    "    mini_train_tensor,\n",
    "    batch_size=args[\"batch_size\"], shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(params, len(ingredients)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-2a70aaddc3da>:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  batches = tqdm(enumerate(train_loader), total=len(train_loader))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b066a26401d8469a8e5dd2e5267d7d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902cd8eefbe24aed853d6f670ffe623f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163486bc8da747aaa2ffb0fa87d6051b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914c761d685d4423b35e66fec356764e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ae75ee34ee4b518269b0183d80f362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(args[\"seed\"])\n",
    "for epoch in range(1, params[\"epochs\"] + 1):\n",
    "    train(model, device, mini_train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
